{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shahzaib\\miniconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import csv\n",
    "from transformers import DebertaV2Model, DebertaV2Config, DebertaV2Tokenizer\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'yangheng/deberta-v3-base-absa'\n",
    "\n",
    "# Suppress specific FutureWarnings and DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_review_id(file_path):\n",
    "    texts = []\n",
    "    review_ids = []\n",
    "    aspect_labels = []\n",
    "\n",
    "    with open(file_path, mode=\"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            texts.append(row['Pre_Text'])\n",
    "            review_ids.append(int(row['Review_ID']))  # Convert to integer\n",
    "            aspect_labels.append(row['aspectCategory'])\n",
    "\n",
    "    return texts, review_ids, aspect_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_with_review_id(texts, review_ids, aspect_labels, tokenizer, max_length):\n",
    "    aspect_label_map = {'food': 0, 'service': 1, 'price': 2, 'ambience': 3, 'views': 4, 'menu': 5, 'staff': 6, 'place': 7, 'drinks': 8, 'location': 9, 'dessert': 10, 'decor': 11, 'clean': 12, 'seating': 13, 'parking': 14}\n",
    "    aspect_labels = [aspect_label_map[aspect] for aspect in aspect_labels]\n",
    "\n",
    "    # Ensure texts are converted to strings\n",
    "    texts = [str(text) for text in texts]\n",
    "\n",
    "    tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = tokenized_texts['input_ids']\n",
    "    attention_masks = tokenized_texts['attention_mask']\n",
    "    aspect_labels = torch.tensor(aspect_labels, dtype=torch.long)  # Convert aspect labels to long tensor\n",
    "    review_ids = torch.tensor(review_ids, dtype=torch.long)  # Convert review IDs to long tensor\n",
    "    return input_ids, attention_masks, aspect_labels, review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeBERTaAspectClassifier(nn.Module):\n",
    "    def __init__(self, num_aspect_labels=15, hidden_size=768, num_filters=64, filter_sizes=[3, 3], dropout=0.1):\n",
    "        super(DeBERTaAspectClassifier, self).__init__()\n",
    "        self.deberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n",
    "        self.aspect_cnn = nn.ModuleList([\n",
    "            nn.Conv1d(hidden_size, num_filters, fs) for fs in filter_sizes])  # Conv1d instead of Conv2d\n",
    "        self.aspect_dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, 128)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(128 + hidden_size, num_aspect_labels)  # Adjusted input size\n",
    "        self.num_aspect_labels = num_aspect_labels\n",
    "        self.aspect_attn = nn.Linear(hidden_size, 1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedded = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        aspect_pooled = []\n",
    "        for conv in self.aspect_cnn:\n",
    "            conv_out = F.relu(conv(embedded.permute(0, 2, 1)))  # Adjust input dimensions\n",
    "            aspect_pooled.append(F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2))\n",
    "        aspect_concat = torch.cat(aspect_pooled, 1)\n",
    "        aspect_concat = self.aspect_dropout(aspect_concat)\n",
    "        # Add aspect attention over DeBERTa embeddings\n",
    "        attn_weights = F.softmax(self.aspect_attn(embedded), dim=1)\n",
    "        aspect_context = torch.sum(embedded * attn_weights, dim=1)\n",
    "        combined = self.fc1(aspect_concat)\n",
    "        combined = F.relu(combined)\n",
    "        # Reshape aspect_context to match aspect_concat\n",
    "        aspect_context = aspect_context.unsqueeze(1).expand(-1, aspect_concat.size(1), -1)\n",
    "        # Repeat aspect_context along the sequence length\n",
    "        aspect_context = aspect_context.unsqueeze(2).repeat(1, 1, embedded.size(1), 1)\n",
    "        combined = torch.cat([combined.unsqueeze(2), aspect_context], dim=3)  # Concatenate along the feature dimension\n",
    "        # Squeeze the combined tensor to remove extra dimensions\n",
    "        combined = combined.squeeze(2)\n",
    "\n",
    "        combined = self.fc2(combined)  # Pass through the final linear layer\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device, num_aspect_labels):\n",
    "    model.eval()\n",
    "    aspect_preds = []\n",
    "    aspect_labels = []\n",
    "    precision_aspect = []\n",
    "    recall_aspect = []\n",
    "    f1_aspect = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, aspect_labels_batch, _ = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            aspect_labels_batch = aspect_labels_batch.to(device)\n",
    "            aspect_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            _, predicted_aspect = torch.max(aspect_logits, dim=1)\n",
    "            aspect_preds.extend(predicted_aspect.tolist())\n",
    "            aspect_labels.extend(aspect_labels_batch.tolist())\n",
    "\n",
    "        for aspect_label in range(num_aspect_labels):\n",
    "            aspect_label_indices = [i for i, label in enumerate(aspect_labels) if label == aspect_label]\n",
    "            aspect_preds_subset = [aspect_preds[i] for i in aspect_label_indices]\n",
    "            aspect_labels_subset = [label for i, label in enumerate(aspect_labels) if i in aspect_label_indices]\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(aspect_labels_subset, aspect_preds_subset, average='weighted', zero_division=1)\n",
    "            precision_aspect.append(precision)\n",
    "            recall_aspect.append(recall)\n",
    "            f1_aspect.append(f1)\n",
    "\n",
    "    return precision_aspect, recall_aspect, f1_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, class_weights=None):\n",
    "\n",
    "    model.to(device)\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, aspect_labels, _ = batch\n",
    "            input_ids, attention_mask, aspect_labels = input_ids.to(device), attention_mask.to(device), aspect_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            aspect_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            if class_weights is not None:\n",
    "                aspect_loss = F.cross_entropy(aspect_logits, aspect_labels, weight=class_weights)\n",
    "            else:\n",
    "                aspect_loss = F.cross_entropy(aspect_logits, aspect_labels)\n",
    "\n",
    "            aspect_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        precision_aspect, recall_aspect, f1_aspect = evaluate(model, val_loader, device, model.num_aspect_labels)  # Pass num_aspect_labels to evaluate function\n",
    "        overall_precision = sum(precision_aspect) / len(precision_aspect)\n",
    "        overall_recall = sum(recall_aspect) / len(recall_aspect)\n",
    "        overall_f1 = sum(f1_aspect) / len(f1_aspect)\n",
    "\n",
    "        # Print the overall precision, recall, and F1-score of the validation data\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Overall Precision: {overall_precision:.3f}, Overall Recall: {overall_recall:.3f}, Overall F1-score: {overall_f1:.3f}')\n",
    "\n",
    "        # Print the results for each aspect\n",
    "        for aspect, precision, recall, f1 in zip(range(len(precision_aspect)), precision_aspect, recall_aspect, f1_aspect):\n",
    "            print(f\"Aspect {aspect}: Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, review_ids, aspect_labels = load_dataset_with_review_id('/content/data.csv')\n",
    "\n",
    "train_texts, val_texts, train_review_ids, val_review_ids, train_aspect_labels, val_aspect_labels = train_test_split(texts, review_ids, aspect_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_length = 500\n",
    "train_input_ids, train_attention_masks, train_aspect_labels, train_review_ids = tokenize_text_with_review_id(train_texts, train_review_ids, train_aspect_labels, tokenizer, max_length)\n",
    "val_input_ids, val_attention_masks, val_aspect_labels, val_review_ids = tokenize_text_with_review_id(val_texts, val_review_ids, val_aspect_labels, tokenizer, max_length)\n",
    "\n",
    "print(f\"Train Data Size: {len(train_texts)}\")\n",
    "print(f\"Test Data Size: {len(val_texts)}\")\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_aspect_labels, train_review_ids)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_aspect_labels, val_review_ids)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = DeBERTaAspectClassifier(num_aspect_labels=15)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 3\n",
    "\n",
    "class_counts = np.bincount(train_aspect_labels)\n",
    "total_samples = len(train_aspect_labels)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights[class_counts == 0] = 1.0\n",
    "class_weights = class_weights / np.sum(class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "torch.cuda.empty_cache()\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, class_weights=class_weights_tensor)\n",
    "\n",
    "# Save the model\n",
    "model_path = '/content/trained_model_aspect_only.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save the model in a zip file\n",
    "with zipfile.ZipFile('/content/trained_model_aspect_only.zip', 'w') as zipf:\n",
    "    zipf.write(model_path, arcname='trained_model_aspect_only.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
